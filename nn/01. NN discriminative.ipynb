{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple Neural Network, using toylib!\n",
    "We repeat the same exercise that we did previously for learning jax, but building `toylib` along this time around!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "from toylib.nn import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702260288.097675   28089 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.\n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = layers.Linear(10, 2, key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up a simple regression problem. The problem is not very meaningful to solve by itself using such a model, but it allows us to get all the pieces in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# problem setup\n",
    "n = 120  # examlpes\n",
    "d = 10  # dimension\n",
    "\n",
    "# dummy data\n",
    "xs = np.random.normal(size=(n, d))\n",
    "ys = np.dot(xs, np.random.randint(0, 10, size=(d,))) + np.random.normal(size=(n,))\n",
    "\n",
    "xs_train, xs_test = xs[:100], xs[100:]\n",
    "ys_train, ys_test = ys[:100], ys[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dummy `ys` are a linear transformation of the inputs. Let's try to fit a model on the train set to predict the values in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss(model, xs, ys):\n",
    "    preds = jax.vmap(model)(xs)\n",
    "    return jax.numpy.mean((ys - preds) ** 2)  # L2 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = layers.Linear(d, 1, use_bias=False, key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(237.21623, dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(model, xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encounter another jax specific error here. The jax.jit function works specficially on pytrees, but the class that we defined is not interpretable as one.\n",
    "\n",
    "We need to fix this.\n",
    "\n",
    "We follow the startegy defined in https://jax.readthedocs.io/en/latest/faq.html#strategy-3-making-customclass-a-pytree. There are other ways to achieve this and altogether different design options possible, we just choose this as a simple and flexible way to make progress.\n",
    "\n",
    "\n",
    "We rely on the abstract class `Module` and register it as a pytree node class.\n",
    "\n",
    "The class needs to distinguish between the `dynamic` elements (which need to interact with jax via `jit` & `grad`) vs the `static` elements:\n",
    "- Hyperparameters (like layer sizes) will be static\n",
    "- The actual weight arrays will be dynamic\n",
    "\n",
    "This is very doable for a single class by manually definiing each of these.\n",
    "\n",
    "For a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training loop\n",
    "# initial estimates\n",
    "params = {\n",
    "    'theta': jax.numpy.array([0, 0], dtype=jax.numpy.float32)\n",
    "}\n",
    "\n",
    "# hyperparmeters\n",
    "max_steps = 100\n",
    "eps = 1e-4\n",
    "\n",
    "step = 0\n",
    "\n",
    "# until we reach max_steps or the loss doesn't change by <eps>\n",
    "while step < max_steps:\n",
    "    # compute model value and grad\n",
    "    (loss, preds), grads = jax.value_and_grad(sum_squared_error, has_aux=True)(params, xs, ys)\n",
    "\n",
    "\n",
    "    print(loss, params)\n",
    "    params_ = update(params, grads)\n",
    "    print(params_)\n",
    "\n",
    "    if jax.numpy.mean(jax.numpy.abs(params['theta'] - params_['theta'])) < eps:\n",
    "        break\n",
    "    params = params_\n",
    "    step += 1\n",
    "    # # check diff \n",
    "    # prev_loss, diff = loss, jax.numpy.abs(prev_loss - loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
