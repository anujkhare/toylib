{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title imports\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import functools\n",
    "import numpy as np\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title func: get_dataloader\n",
    "def get_dataloader(dataset, batch_size: int = 16, shuffle: bool = True) -> DataLoader:\n",
    "  return DataLoader(\n",
    "      dataset=dataset,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=shuffle\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load the datasets and create dataloaders\n",
    "batch_size = 32  #@param\n",
    "shuffle = True  #@param\n",
    "download = True  #@param\n",
    "\n",
    "image_transforms = transforms.Compose([\n",
    "  transforms.Pad(padding=2, fill=0, padding_mode=\"constant\"),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize((0.5), (.5)),\n",
    "])\n",
    "\n",
    "dataset_train = MNIST(root=\"data/MNIST/train\", train=True, download=download, transform=image_transforms)\n",
    "dataset_test = MNIST(root=\"data/MNIST/test\", train=False, download=download, transform=image_transforms)\n",
    "\n",
    "dataloader_train = get_dataloader(dataset_train, batch_size=batch_size, shuffle=shuffle)\n",
    "dataloader_test = get_dataloader(dataset_test, batch_size=batch_size, shuffle=shuffle, )\n",
    "\n",
    "print(f\"Train samples: {len(dataset_train)} ; {len(dataloader_train)} batches\")\n",
    "print(f\"Test samples: {len(dataset_test)} ; {len(dataloader_test)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_train[0][0].mean(), dataset_train[0][0].std())\n",
    "print(dataset_train[1][0].min(), dataset_train[1][0].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title classes: Encoder, CNNDiscriminator\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, n_c_in, n_c_out, p_dropout=0):\n",
    "        super().__init__()\n",
    "        self.b1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(n_c_in, n_c_out, kernel_size=3, stride=1, padding=1),\n",
    "            # torch.nn.BatchNorm2d(n_c_out),\n",
    "            torch.nn.LeakyReLU(0.2),\n",
    "            torch.nn.Dropout(p_dropout),\n",
    "        )\n",
    "        # self.b2 = torch.nn.Sequential(\n",
    "        #     torch.nn.Conv2d(n_c_out, n_c_out, kernel_size=3, stride=1, padding=1),\n",
    "        #     torch.nn.BatchNorm2d(n_c_out),\n",
    "        #     torch.nn.ReLU(inplace=True),\n",
    "        #     torch.nn.Dropout(p_dropout),\n",
    "        # )\n",
    "        # self.b3 = torch.nn.Sequential(\n",
    "        #     torch.nn.Conv2d(n_c_out, n_c_out, kernel_size=3, stride=1, padding=1),\n",
    "        #     torch.nn.BatchNorm2d(n_c_out),\n",
    "        #     torch.nn.ReLU(inplace=True),\n",
    "        #     torch.nn.Dropout(p_dropout),\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.b1(x)\n",
    "        return out1\n",
    "        # r1 = out1  # no residual since input is different size\n",
    "        # out2 = self.b2(r1)\n",
    "        # r2 = r1 + out2\n",
    "        # out3 = self.b3(r2)\n",
    "        # r3 = r2 + out3\n",
    "        # return r3\n",
    "\n",
    "class CNNDiscriminator(torch.nn.Module):\n",
    "  \"\"\"\n",
    "  A very simple CNN with \"resnet\" style blocks (not even sure if they truly are)\n",
    "  \"\"\"\n",
    "  def __init__(self, n_channels, n_classes):\n",
    "    super().__init__()\n",
    "    self.encoder1 = Encoder(n_channels, 16)\n",
    "    self.encoder2 = Encoder(16, 32)\n",
    "    self.encoder3 = Encoder(32, 64)\n",
    "    self.classifier = torch.nn.Linear(16 * 64, n_classes)\n",
    "\n",
    "  @staticmethod\n",
    "  def _encode_and_pool(encoder, inputs):\n",
    "    encoded = encoder(inputs)\n",
    "    pooled = torch.nn.functional.max_pool2d(encoded, kernel_size=2, stride=2)\n",
    "    return encoded, pooled\n",
    "\n",
    "  def forward(self, x):\n",
    "    encoded1, pooled1 = self._encode_and_pool(self.encoder1, x)\n",
    "    encoded2, pooled2 = self._encode_and_pool(self.encoder2, pooled1)\n",
    "    encoded3, pooled3 = self._encode_and_pool(self.encoder3, pooled2)\n",
    "    activations = self.classifier(pooled3.view(pooled3.shape[0], -1))\n",
    "    probs = torch.nn.functional.softmax(activations, dim=-1)\n",
    "    log_probs = torch.nn.functional.log_softmax(activations, dim=-1)\n",
    "    return log_probs, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "  device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title MNIST model,loss,optim\n",
    "model_mnist = CNNDiscriminator(1, 10).to(device)\n",
    "n_params = sum([np.prod(param.size()) for param in model_mnist.parameters()])\n",
    "print(f\"{n_params} parameters\")\n",
    "\n",
    "# Test the model\n",
    "images, labels = next(iter(dataloader_train))\n",
    "print(images.shape, labels.shape)\n",
    "_, pred_probs = model_mnist(images.to(device))\n",
    "print(pred_probs.shape)\n",
    "\n",
    "# Loss & optim\n",
    "loss_mnist = torch.nn.NLLLoss().to(device)\n",
    "optimizer_mnist = torch.optim.Adam(params=model_mnist.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title visualizing funcs\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from skimage.util import montage\n",
    "\n",
    "\n",
    "# def show_img(im, figsize=None, ax=None, title=None):\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     if not ax: fig, ax = plt.subplots(figsize=figsize)\n",
    "#     ax.imshow(im, cmap='gray')\n",
    "#     if title is not None: ax.set_title(title, fontsize=50)\n",
    "#     ax.get_xaxis().set_visible(False)\n",
    "#     ax.get_yaxis().set_visible(False)\n",
    "#     return ax\n",
    "\n",
    "\n",
    "# def draw_rect(ax, bbox, edgecolor='red'):\n",
    "#     import matplotlib.patches as patches\n",
    "#     x, y, w, h = bbox\n",
    "#     patch = ax.add_patch(patches.Rectangle((x, y), w, h, fill=False, edgecolor=edgecolor, lw=2))\n",
    "\n",
    "\n",
    "# def draw_canvas(img, bboxes: np.ndarray, color='red'):\n",
    "#     fig, ax = plt.subplots(1, 1, figsize=(192 / 20, 108 / 20))\n",
    "\n",
    "#     for ix in range(len(bboxes)):\n",
    "#         bbox = bboxes[ix]\n",
    "\n",
    "#         draw_rect(ax, bbox, edgecolor=color)  # will add red bounding boxes for each character\n",
    "\n",
    "#     ax = show_img(img, ax=ax)\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "\n",
    "\n",
    "# def visualize_data(dataloader, n=5, fields_to_print=None):\n",
    "#     for ix, data in enumerate(dataloader):\n",
    "#         if ix >= n:\n",
    "#             break\n",
    "\n",
    "#         img = data[\"image\"][0].data.cpu().numpy().transpose(1, 2, 0)\n",
    "#         dist = data[\"dists\"][0].data.cpu().numpy().astype(np.uint8)\n",
    "#         word = data[\"labels\"][0].data.cpu().numpy().astype(np.float32)\n",
    "\n",
    "#         if fields_to_print is not None:\n",
    "#             for field in fields_to_print:\n",
    "#                 print(data[field])\n",
    "\n",
    "#         # Remove the -100s in the padding\n",
    "#         pad_idx = np.where(word == -100)\n",
    "#         dist[pad_idx] = MAX_DIST - 1\n",
    "#         word[pad_idx] = 0\n",
    "\n",
    "#         print(img.shape, dist.shape)\n",
    "#         plt.figure(figsize=(20, 5))\n",
    "#         plt.imshow(img.astype(np.uint8))\n",
    "#         plt.show()\n",
    "#         plt.figure(figsize=(20, 5))\n",
    "#         plt.title(np.unique(dist))\n",
    "#         plt.imshow(dist.astype(np.uint8))\n",
    "#         plt.show()\n",
    "#         plt.figure(figsize=(20, 5))\n",
    "#         plt.title(np.unique(word))\n",
    "#         plt.imshow(word.astype(np.float32))\n",
    "#         plt.show()\n",
    "#         print('--------------')\n",
    "\n",
    "\n",
    "# def visualize_data_outputs(images, target_label, target_dist, pred_labels, pred_dists):\n",
    "#     pad_idxs = np.where(target_dist == -100)\n",
    "#     target_label[pad_idxs] = 0\n",
    "#     pred_labels[pad_idxs] = 0\n",
    "#     target_dist[pad_idxs] = MAX_DIST - 1\n",
    "#     pred_dists[pad_idxs] = MAX_DIST - 1\n",
    "\n",
    "#     images = images.astype(np.int)\n",
    "\n",
    "#     ndim = images.shape[0]\n",
    "#     if ndim > 1:\n",
    "#         images = montage(images, multichannel=True)\n",
    "#         target_dist = montage(target_dist)\n",
    "#         target_label = montage(target_label)\n",
    "#         pred_labels = montage(pred_labels)\n",
    "#         pred_dists = montage(pred_dists)\n",
    "#     else:\n",
    "#         images = images[0]\n",
    "#         target_dist = target_dist[0]\n",
    "#         target_label = target_label[0]\n",
    "#         pred_labels = pred_labels[0]\n",
    "#         pred_dists = pred_dists[0]\n",
    "\n",
    "#     N = 5\n",
    "#     plt.figure(figsize=(20, 10))\n",
    "#     plt.subplot(1, N, 1)\n",
    "#     plt.imshow(images)\n",
    "#     plt.title('Input images')\n",
    "\n",
    "#     plt.subplot(1, N, 2)\n",
    "#     plt.imshow(target_label)\n",
    "#     plt.title('Target labels')\n",
    "\n",
    "#     plt.subplot(1, N, 3)\n",
    "#     plt.imshow(pred_labels)\n",
    "#     plt.title('Predicted labels')\n",
    "\n",
    "#     plt.subplot(1, N, 4)\n",
    "#     plt.imshow(target_dist)\n",
    "#     plt.title('Target dists')\n",
    "\n",
    "#     plt.subplot(1, N, 5)\n",
    "#     plt.imshow(pred_dists)\n",
    "#     plt.title('Predicted dists')\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = f\"zcode/tb-logs/vanilla-gan-{version}\"\n",
    "logger_d = SummaryWriter(logdir=log_dir)\n",
    "print(f\"logging to {log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"{log_dir}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / val functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title func: validate\n",
    "def validate(dataloader, n_batches, model, loss_func, log_func=None):\n",
    "  model.train(False)\n",
    "  model.eval()\n",
    "  device = next(model.parameters()).device\n",
    "\n",
    "  loss = 0\n",
    "  n_correct, n_total = 0, 0\n",
    "  for ix, batch_val in enumerate(dataloader):\n",
    "    if ix >= n_batches: break\n",
    "    images_val, labels_val = batch_val\n",
    "\n",
    "    # Get the loss\n",
    "    log_probs, _ = model(images_val.to(device))\n",
    "    loss += loss_func(log_probs, labels_val.to(device)).data.cpu().numpy()\n",
    "\n",
    "    # Get the acc\n",
    "    preds = np.argmax(log_probs.data.cpu().numpy(), axis=-1)\n",
    "    n_correct += np.sum(preds == labels_val)\n",
    "    n_total += labels_val.shape[0]\n",
    "\n",
    "    # if ix == 0:\n",
    "    #   image_montage = montage(images_val.squeeze().data.cpu().numpy())\n",
    "    #   plt.figure(figsize=(5, 5))\n",
    "    #   plt.subplot(1, 1, 1)\n",
    "    #   plt.imshow(image_montage)\n",
    "    #   plt.title(f\"Labels: {labels_val.data.cpu().numpy()}\\nPreds: {preds}\")\n",
    "    #   plt.show()\n",
    "\n",
    "  model.train(True)\n",
    "  loss /= n_batches\n",
    "  if log_func:\n",
    "    log_func(tag=\"disc.loss.val\", scalar_value=loss)\n",
    "    log_func(tag=\"disc.acc.val\", scalar_value=n_correct/n_total)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "conv_general_dilated lhs feature dimension size divided by feature_group_count must equal the rhs input feature dimension size, but 12 // 1 != 4.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m12\u001b[39m))\n\u001b[1;32m      2\u001b[0m k \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVALID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/toylib2/lib/python3.9/site-packages/jax/_src/lax/convolution.py:196\u001b[0m, in \u001b[0;36mconv\u001b[0;34m(lhs, rhs, window_strides, padding, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconv\u001b[39m(lhs: Array, rhs: Array, window_strides: Sequence[\u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    175\u001b[0m          padding: \u001b[38;5;28mstr\u001b[39m, precision: lax\u001b[38;5;241m.\u001b[39mPrecisionLike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    176\u001b[0m          preferred_element_type: DTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[1;32m    177\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Convenience wrapper around `conv_general_dilated`.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m    An array containing the convolution result.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconv_general_dilated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_strides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/toylib2/lib/python3.9/site-packages/jax/_src/lax/convolution.py:161\u001b[0m, in \u001b[0;36mconv_general_dilated\u001b[0;34m(lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation, dimension_numbers, feature_group_count, batch_group_count, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpadding argument to conv_general_dilated should be a string or a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence of (low, high) pairs, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpadding\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    158\u001b[0m preferred_element_type \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m preferred_element_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(np\u001b[38;5;241m.\u001b[39mdtype(preferred_element_type)))\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconv_general_dilated_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_strides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwindow_strides\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlhs_dilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlhs_dilation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs_dilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrhs_dilation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdimension_numbers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdnums\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_group_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_group_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_group_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_group_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanonicalize_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/toylib2/lib/python3.9/site-packages/jax/_src/core.py:416\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    414\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39menable_checks\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    415\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[0;32m--> 416\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_top_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/toylib2/lib/python3.9/site-packages/jax/_src/core.py:420\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[1;32m    419\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m pop_level(trace\u001b[38;5;241m.\u001b[39mlevel):\n\u001b[0;32m--> 420\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/toylib2/lib/python3.9/site-packages/jax/_src/core.py:921\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    919\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m call_impl_with_key_reuse_checks(primitive, primitive\u001b[38;5;241m.\u001b[39mimpl, \u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 921\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/toylib2/lib/python3.9/site-packages/jax/_src/dispatch.py:87\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m     85\u001b[0m prev \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m   outs \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m   lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(prev)\n",
      "    \u001b[0;31m[... skipping hidden 18 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/toylib2/lib/python3.9/site-packages/jax/_src/lax/convolution.py:374\u001b[0m, in \u001b[0;36m_conv_general_dilated_shape_rule\u001b[0;34m(lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation, dimension_numbers, feature_group_count, batch_group_count, **unused_kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m core\u001b[38;5;241m.\u001b[39mdefinitely_equal(quot, rhs\u001b[38;5;241m.\u001b[39mshape[dimension_numbers\u001b[38;5;241m.\u001b[39mrhs_spec[\u001b[38;5;241m1\u001b[39m]]):\n\u001b[1;32m    371\u001b[0m   msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv_general_dilated lhs feature dimension size divided by \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_group_count must equal the rhs input feature dimension \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    373\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize, but \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m // \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 374\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(lhs_feature_count, feature_group_count,\n\u001b[1;32m    375\u001b[0m                               rhs\u001b[38;5;241m.\u001b[39mshape[dimension_numbers\u001b[38;5;241m.\u001b[39mrhs_spec[\u001b[38;5;241m1\u001b[39m]]))\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rhs\u001b[38;5;241m.\u001b[39mshape[dimension_numbers\u001b[38;5;241m.\u001b[39mrhs_spec[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m%\u001b[39m feature_group_count:\n\u001b[1;32m    377\u001b[0m   msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv_general_dilated rhs output feature dimension size must be a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    378\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiple of feature_group_count, but \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not a multiple of \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: conv_general_dilated lhs feature dimension size divided by feature_group_count must equal the rhs input feature dimension size, but 12 // 1 != 4."
     ]
    }
   ],
   "source": [
    "a = np.zeros((12, 12))\n",
    "k = np.ones((4, 4))\n",
    "jax.lax.conv(a, k, (3, 3), \"VALID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.lax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title func: train\n",
    "import math\n",
    "\n",
    "\n",
    "def train(dataloader, n_epochs, model, loss_func, optimizer,\n",
    "          logger, callback_frequency=100, callbacks=None,\n",
    "          es_smoothing=0.6, es_threshold=1e-8\n",
    "          ) -> None:\n",
    "  # Max steps to run for (in case of fractional n_epochs)\n",
    "  max_steps = len(dataloader)\n",
    "  if n_epochs < 1:\n",
    "    max_steps = math.ceil(n_epochs * max_steps)\n",
    "    n_epochs = 1\n",
    "\n",
    "  if callbacks is None:\n",
    "    callbacks = []\n",
    "\n",
    "  device = next(model.parameters()).device\n",
    "\n",
    "  # Running loss - for early stopping\n",
    "  average_loss = 0\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "    for ix, batch_train in enumerate(dataloader):\n",
    "      step = ix + (epoch * len(dataloader))\n",
    "      if ix >= max_steps:\n",
    "        break\n",
    "    \n",
    "      # Get a train batch\n",
    "      images_train, labels_train = batch_train\n",
    "      \n",
    "      # Get preds\n",
    "      pred_log_probs_train, pred_probs_train = model(images_train.to(device))\n",
    "      \n",
    "      # calculate loss\n",
    "      optimizer.zero_grad()\n",
    "      loss_d_train = loss_func(pred_log_probs_train, labels_train.to(device))\n",
    "      \n",
    "      # backward\n",
    "      loss_d_train.backward()\n",
    "      optimizer.step()\n",
    "    \n",
    "      loss_val = loss_d_train.data.cpu().numpy()\n",
    "      # tensorboard logging\n",
    "      log_func = functools.partial(logger.add_scalar, global_step=step)\n",
    "      log_func(tag=\"disc.loss.train\", scalar_value=loss_val)\n",
    "\n",
    "      # early stopping\n",
    "      if np.abs(loss_val - average_loss) < es_threshold:\n",
    "        print(f\"Stopped early at iteration {step} with {loss_val}, average: {average_loss}\")\n",
    "        return\n",
    "      average_loss = average_loss * es_smoothing + loss_val * (1 - es_smoothing)\n",
    "\n",
    "      # Call the callbacks!\n",
    "      if ix % callback_frequency == 0:\n",
    "        _ = [f(model=model, log_func=log_func, loss_func=loss_func) for f in callbacks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title class: Deconv, Generator\n",
    "class Deconv(torch.nn.Module):\n",
    "    def __init__(self, n_c_in, n_c_out):\n",
    "        super().__init__()\n",
    "        self.b1 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(n_c_in, n_c_out, kernel_size=2, stride=2, padding=0),\n",
    "            torch.nn.BatchNorm2d(n_c_out),\n",
    "            torch.nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.b1(x)\n",
    "\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, p_dropout=0):\n",
    "        super().__init__()\n",
    "        # deconv are the set of layers that enlarge the encoded image to double size\n",
    "        self.deconv1 = Deconv(64, 32)\n",
    "        self.deconv2 = Deconv(32, 32)\n",
    "        self.deconv3 = Deconv(32, 16)\n",
    "        # decoder uses deconvolution to creates the segmented image from encoded images using U-net structure\n",
    "        self.decoder1 = Encoder(32, 32, p_dropout)\n",
    "        self.decoder2 = Encoder(32, 32, p_dropout)\n",
    "        self.decoder3 = Encoder(16, 16, p_dropout)\n",
    "        self.word_pred = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 1, kernel_size=(1, 1), stride=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        deconved1 = self.deconv1(inputs)\n",
    "        decoded1 = self.decoder1(deconved1)\n",
    "        deconved2 = self.deconv2(decoded1)\n",
    "        decoded2 = self.decoder2(deconved2)\n",
    "        deconved3 = self.deconv3(decoded2)\n",
    "        decoded3 = self.decoder3(deconved3)\n",
    "        activations = self.word_pred(decoded3)\n",
    "        return torch.tanh(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Generator configs\n",
    "batch_size_g =   32#@param\n",
    "n_epochs_g =   1#@param\n",
    "\n",
    "n_epochs_d =   1#@param\n",
    "batch_size_d = 32  #@param\n",
    "n_cycles = 700  #@param\n",
    "\n",
    "half_bsz = batch_size_d // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title class: RealGenDataset\n",
    "class RealGenDataset(torch.utils.data.Dataset):\n",
    "  \"\"\"\n",
    "  Discriminator Labels:\n",
    "    - Generated: 1\n",
    "    - Real: 0\n",
    "  \n",
    "  If using with \"only_generated\", I flip the labels:\n",
    "    - Generated: 0\n",
    "  \n",
    "  This is simply done for easy code reuse while training the generator since\n",
    "  we want the discriminator to think that the generated images are real\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, dataset_real, model_gen, only_generated=False):\n",
    "    self.dataset = dataset_real\n",
    "    self.generator = model_gen\n",
    "    self.device = next(model_gen.parameters()).device\n",
    "    self.only_generated = only_generated\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "  \n",
    "  def __getitem__(self, ix, rng=None):\n",
    "    image_real, _ = self.dataset[ix]\n",
    "\n",
    "    if rng is not None:\n",
    "      np.random.seed(rng)\n",
    "    \n",
    "    z = torch.from_numpy(np.random.randn(1, 64, 4, 4).astype(np.float32))\n",
    "    # Note: still attached to the graph\n",
    "    image_gen = self.generator(z.to(self.device)).cpu() \n",
    "\n",
    "    if self.only_generated:\n",
    "      return image_gen, torch.from_numpy(np.array([0]).astype(np.long))\n",
    "\n",
    "    images = torch.cat([image_real.view(1, *image_real.shape), image_gen], dim=0)\n",
    "    labels = torch.from_numpy(np.array([0, 1]).astype(np.long))\n",
    "    return (images, labels)\n",
    "\n",
    "def collate_batches(batches):\n",
    "  images = torch.cat([b[0] for b in batches], dim=0)\n",
    "  labels = torch.cat([b[1] for b in batches], dim=0)\n",
    "  return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_g = Generator().to(device)\n",
    "model_d = CNNDiscriminator(n_channels=1, n_classes=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title get dataset_train_d dataloader_train_d dataset_test_d dataloader_test_d\n",
    "\n",
    "dataset_train_d = RealGenDataset(dataset_train, model_g)\n",
    "dataloader_train_d = DataLoader(\n",
    "    dataset_train_d,\n",
    "    batch_size=half_bsz,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batches\n",
    "    )\n",
    "\n",
    "dataset_test_d = RealGenDataset(dataset_test, model_g)\n",
    "dataloader_test_d = DataLoader(\n",
    "    dataset_test_d,\n",
    "    batch_size=half_bsz,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batches\n",
    "    )\n",
    "\n",
    "# Check if it makes sense\n",
    "images, labels = next(iter(dataloader_train_d))\n",
    "print(images.shape, labels.shape, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title get dataset_train_g dataloader_train_g dataset_test_g dataloader_test_g\n",
    "\n",
    "dataset_train_g = RealGenDataset(dataset_train, model_g, only_generated=True)\n",
    "dataloader_train_g = DataLoader(\n",
    "    dataset_train_g,\n",
    "    batch_size=batch_size_g,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batches\n",
    "    )\n",
    "\n",
    "dataset_test_g = RealGenDataset(dataset_test, model_g, only_generated=True)\n",
    "dataloader_test_g = DataLoader(\n",
    "    dataset_test_g,\n",
    "    batch_size=batch_size_g,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batches\n",
    "    )\n",
    "\n",
    "# Check if it makes sense\n",
    "images, labels = next(iter(dataloader_train_g))\n",
    "print(images.shape, labels.shape, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title loss functions and optimizers\n",
    "loss_d = torch.nn.NLLLoss()\n",
    "loss_g = torch.nn.NLLLoss()\n",
    "optimizer_d = torch.optim.Adam(model_d.parameters())\n",
    "optimizer_g = torch.optim.Adam(model_g.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_batch(batch, model, optimizer, loss_func):\n",
    "  # Get a train batch\n",
    "  images_train, labels_train = batch\n",
    "  \n",
    "  # Get preds\n",
    "  pred_log_probs_train, pred_probs_train = model(images_train.to(device))\n",
    "  \n",
    "  # calculate loss\n",
    "  optimizer.zero_grad()\n",
    "  loss_d_train = loss_func(pred_log_probs_train, labels_train.to(device))\n",
    "  \n",
    "  # backward\n",
    "  loss_d_train.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  return loss_d_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_foo():\n",
    "  print(f\"G: {next(model_g.parameters()).mean(), next(model_g.parameters()).std()}\")\n",
    "  print(f\"D: {next(model_d.parameters()).mean(), next(model_d.parameters()).std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title train gan\n",
    "import time\n",
    "\n",
    "val_func_d = functools.partial(validate, dataloader=dataloader_test_d, n_batches=5)\n",
    "val_func_g = functools.partial(validate, dataloader=dataloader_test_g, n_batches=5)\n",
    "\n",
    "step =   0   ##@param\n",
    "start_time = time.time()\n",
    "while True:\n",
    "  step += 1\n",
    "\n",
    "  # Train the discrimator on one batch\n",
    "  model_d.train(True)\n",
    "  model_g.train(False)\n",
    "  loss_train_d = train_one_batch(\n",
    "      batch=next(iter(dataloader_train_d)),\n",
    "      model=model_d,\n",
    "      optimizer=optimizer_d,\n",
    "      loss_func=loss_d\n",
    "  )\n",
    "  logger_d.add_scalar(\"dis.loss.train\", loss_train_d.data.cpu().numpy(), step)\n",
    "\n",
    "  # Train the generator on one batch\n",
    "  model_d.train(False)\n",
    "  model_g.train(True)\n",
    "  loss_train_g = train_one_batch(\n",
    "      batch=next(iter(dataloader_train_g)),\n",
    "      model=model_d,\n",
    "      optimizer=optimizer_g,\n",
    "      loss_func=loss_g\n",
    "  )\n",
    "  logger_d.add_scalar(\"gen.loss.train\", loss_train_g.data.cpu().numpy(), step)\n",
    "\n",
    "  # # Validation \n",
    "  # if step % 200 == 0:\n",
    "  #   loss_val_g = val_func_g(model=model_d, loss_func=loss_g)\n",
    "  #   loss_val_d = val_func_d(model=model_d, loss_func=loss_d)\n",
    "  #   logger_d.add_scalar(\"gen.loss.val\", loss_val_g, step)\n",
    "  #   logger_d.add_scalar(\"dis.loss.val\", loss_val_d, step)\n",
    "  \n",
    "  # # Save\n",
    "  # if step % 2000 == 0:\n",
    "  #   torch.save(model_d, log_dir + f\"model_d_{step}.pt\")\n",
    "  #   torch.save(model_g, log_dir + f\"model_g_{step}.pt\")\n",
    "  #   print(f\"seconds elapsed since last checkpoint: {time.time() - start_time}\")\n",
    "  #   start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_d, log_dir + f\"model_d_{step}.pt\")\n",
    "torch.save(model_g, log_dir + f\"model_g_{step}.pt\")\n",
    "print(f\"seconds elapsed since last checkpoint: {time.time() - start_time}\")\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
