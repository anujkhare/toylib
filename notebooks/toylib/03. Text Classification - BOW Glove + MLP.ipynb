{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1S9viVuVaQi"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "from datasets import load_dataset\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorboardX import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import chex\n",
        "import copy\n",
        "import dataclasses\n",
        "import functools\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import nltk\n",
        "import numpy as np\n",
        "import optax\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from toylib import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('punkt', download_dir='~/data/nltk_data')\n",
        "nltk.download('punkt_tab', download_dir='~/data/nltk_data')\n",
        "\n",
        "nltk.data.path.append('~/data/nltk_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4Vf1onlcX4J"
      },
      "outputs": [],
      "source": [
        "#@title General hyperparms\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class Config:\n",
        "    batch_size: int = 128\n",
        "    embedding_dim: int = 100\n",
        "    max_tokens: int = 160\n",
        "\n",
        "    embeddings_path: str = 'glove.6B.100d.txt'\n",
        "\n",
        "    # training loop hyperparms\n",
        "    num_epochs: int = 5\n",
        "    learning_rate: float = 1e-2\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zE3KSi2QVyiP"
      },
      "outputs": [],
      "source": [
        "#@title Dataset Loaders\n",
        "def preprocess_text(text: str, stopwords = None):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove special characters and replace with space\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
        "    \n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords if provided\n",
        "    if stopwords:\n",
        "        tokens = [word for word in tokens if word not in stopwords]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def embed_tokens(tokens, glove, embedding_dim):\n",
        "    vectors = []\n",
        "    emb_missing = []\n",
        "    for token in tokens:\n",
        "        if token in glove:\n",
        "            vectors.append(glove[token])\n",
        "            emb_missing.append(False)\n",
        "        else:\n",
        "            vectors.append(np.zeros(embedding_dim))\n",
        "            emb_missing.append(True)\n",
        "            print(f\"Token not found in glove: {token}\")\n",
        "    return np.stack(vectors), np.array(emb_missing)\n",
        "\n",
        "\n",
        "class EmbeddedTextDataset(Dataset):\n",
        "    def __init__(self, hf_dataset, glove, embedding_dim, max_tokens=50):\n",
        "        self.data = hf_dataset\n",
        "        self.glove = glove\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.max_tokens = max_tokens\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    @property\n",
        "    def stopwords(self) -> list[str]:\n",
        "        stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "\n",
        "        # specific stopwords\n",
        "        specific_sw = ['br', 'movie', 'film']\n",
        "\n",
        "        # all stopwords\n",
        "        stopwords = stopwords + specific_sw\n",
        "        return stopwords\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        tokens = preprocess_text(item[\"text\"])[:self.max_tokens]\n",
        "        emb, emb_missing = embed_tokens(tokens, self.glove, self.embedding_dim)\n",
        "\n",
        "        # Pad or truncate\n",
        "        num_pad = 0\n",
        "        if emb.shape[0] < self.max_tokens:\n",
        "            num_pad = self.max_tokens - emb.shape[0]\n",
        "\n",
        "            # pad embeddings\n",
        "            pad = np.zeros((num_pad, self.embedding_dim))\n",
        "            emb = np.vstack([emb, pad])\n",
        "            # pad mask\n",
        "            emb_missing = np.concatenate([emb_missing, np.ones(num_pad).astype(np.bool)])\n",
        "        else:\n",
        "            emb = emb[:self.max_tokens]\n",
        "            emb_missing = emb_missing[:self.max_tokens]\n",
        "\n",
        "        return {\n",
        "            \"embedding\": emb.astype(np.float32),\n",
        "            \"embedding_missing\": emb_missing.astype(np.bool),\n",
        "            \"num_tokens\": len(tokens),\n",
        "            \"raw_text\": item[\"text\"],\n",
        "            \"label\": item[\"label\"],\n",
        "            \"num_pad\": num_pad,\n",
        "        }\n",
        "\n",
        "def numpy_collate(batch):\n",
        "    return {\n",
        "        \"embedding\": np.stack([item[\"embedding\"] for item in batch]),   # (B, T, D),\n",
        "        \"embedding_missing\": np.stack([item[\"embedding_missing\"] for item in batch]),   # (B, T),\n",
        "        \"label\": np.array([item[\"label\"] for item in batch], dtype=np.float32),   # (B,)\n",
        "        \"num_pad\": np.array([item[\"num_pad\"] for item in batch]),   # (B,)\n",
        "        \"num_tokens\": np.array([item[\"num_tokens\"] for item in batch]),   # (B,)\n",
        "        \"raw_text\": np.array([item[\"raw_text\"] for item in batch]),   # (B,)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moyQLteIenvo"
      },
      "outputs": [],
      "source": [
        "#@title Embeddings loader\n",
        "def load_glove_embeddings(filepath):\n",
        "    embeddings = {}\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            word = parts[0]\n",
        "            vector = np.array(parts[1:], dtype=np.float32)\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "glove = load_glove_embeddings(config.embeddings_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxsA1NtfXxRe"
      },
      "outputs": [],
      "source": [
        "#@title Set up dataloaders: use IMDB movie sentiment reviews from HuggingFace datasets\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# Create a validation set from the train set\n",
        "train_val = imdb_dataset[\"train\"].train_test_split(test_size=0.05, seed=42)\n",
        "\n",
        "dataset_fn = functools.partial(EmbeddedTextDataset, glove=glove, embedding_dim=config.embedding_dim, max_tokens=config.max_tokens)\n",
        "\n",
        "train_dataset = dataset_fn(train_val['train'])\n",
        "val_dataset = dataset_fn(train_val['test'])\n",
        "test_dataset = dataset_fn(['test'])\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=numpy_collate)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=numpy_collate)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=numpy_collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'Training batches: {len(train_dataset) / config.batch_size}')\n",
        "print(f'Val batches: {len(val_dataset) / config.batch_size}')\n",
        "print(f'Test batches: {len(test_dataset) / config.batch_size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "012zDTfjWRQy",
        "outputId": "76494fd1-3c93-4410-ef55-8657b5da9a62"
      },
      "outputs": [],
      "source": [
        "#@title Visualize a batch of data\n",
        "def visualize_samples(batch) -> pd.DataFrame:\n",
        "    missing = batch['embedding_missing'].sum(axis=1) - batch['num_pad']\n",
        "    return pd.DataFrame({\n",
        "        'labels': batch['label'],\n",
        "        'text': batch['raw_text'],\n",
        "        'missing': missing,\n",
        "        'num_tokens': batch['num_tokens'],\n",
        "    })\n",
        "\n",
        "batch = next(iter(train_dataloader))\n",
        "visualize_samples(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocess_text(batch['raw_text'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45J_p5V0cM0Q"
      },
      "outputs": [],
      "source": [
        "#@title Define a model\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class BagOfWordsClassifier(nn.module.Module):\n",
        "    hidden_sizes: list[int]\n",
        "    output_dim: int = 1\n",
        "    input_dim: int = 100\n",
        "\n",
        "    def __init__(self, hidden_sizes: list[int], input_dim: int, output_dim: int, *, key: jax.random.PRNGKey) -> None:\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "\n",
        "        self.layers = []\n",
        "        in_features = input_dim\n",
        "        for hidden_size in hidden_sizes:\n",
        "            key_used, key = jax.random.split(key, 2)\n",
        "            self.layers.append(nn.layers.Linear(in_features, hidden_size, key=key_used))\n",
        "            in_features = hidden_size\n",
        "\n",
        "        self.output_layer = nn.layers.Linear(in_features, output_dim, key=key)\n",
        "\n",
        "    def __call__(self, x: np.ndarray, mask: np.ndarray) -> ...:\n",
        "        chex.assert_rank(x, 2)  # [num_tokens, embed_dim] - batch should be vmapped\n",
        "        chex.assert_shape(mask, (x.shape[0],))\n",
        "\n",
        "        embed_dim = x.shape[-1]\n",
        "        print(x.shape)\n",
        "\n",
        "        # average the token embeddings to form a \"sentence\" embedding\n",
        "        x = jnp.sum(x * mask[:, np.newaxis], axis=0) / jnp.sum(mask, axis=0)\n",
        "        chex.assert_shape(x, (embed_dim,))  # [embed_dim]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "            x = jax.nn.relu(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9Y8pzHbhXrg"
      },
      "outputs": [],
      "source": [
        "def loss_fn(model, batch):\n",
        "    x, y, mask = batch[\"embedding\"], batch[\"label\"], ~batch[\"embedding_missing\"]\n",
        "    logits = jax.vmap(model)(x, mask)\n",
        "\n",
        "    # Binary cross entropy from logits\n",
        "    log_probs = jax.nn.log_sigmoid(logits)\n",
        "    log_1minus_probs = jax.nn.log_sigmoid(-logits)  # log(1-sigmoid(x)) = log_sigmoid(-x)\n",
        "\n",
        "    loss = -(y * log_probs + (1 - y) * log_1minus_probs)\n",
        "\n",
        "    return jnp.mean(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bow_model = BagOfWordsClassifier([256, 256], 100, 1, key=jax.random.PRNGKey(1))\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optax.adam(learning_rate=config.learning_rate)\n",
        "opt_state = optimizer.init(bow_model)\n",
        "\n",
        "# Value and gradient\n",
        "loss_and_grad_fn = jax.value_and_grad(loss_fn)\n",
        "\n",
        "# TensorBoard writer\n",
        "writer = SummaryWriter(logdir=\"./runs/\" + time.strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "# Training loop\n",
        "step = 0\n",
        "# for epoch in range(config.num_epochs):\n",
        "orig_model = copy.deepcopy(bow_model)\n",
        "for epoch in range(200):\n",
        "    for  batch in train_dataloader:\n",
        "        \n",
        "        loss_val, grads = loss_and_grad_fn(bow_model, batch)\n",
        "        \n",
        "        # Apply gradients\n",
        "        updates, opt_state = optimizer.update(grads, opt_state)\n",
        "        leaves, _ = jax.tree_util.tree_flatten(updates)\n",
        "        bow_model = optax.apply_updates(bow_model, updates)\n",
        "\n",
        "        # Log to TensorBoard\n",
        "        writer.add_scalar(\"train/loss\", float(loss_val), step)\n",
        "        writer.add_scalar(\"train/learning_rate\", config.learning_rate, step)\n",
        "        writer.add_scalar(\"gradients/0/mean\", leaves[0].mean(), step)\n",
        "        writer.add_scalar(\"gradients/1/mean\", leaves[1].mean(), step)\n",
        "        writer.add_scalar(\"gradients/2/mean\", leaves[2].mean(), step)\n",
        "\n",
        "        num_missing = np.mean(batch['embedding_missing'].sum(axis=1) - batch['num_pad'])\n",
        "        writer.add_scalar(\"data/padding\", batch['num_pad'].mean(), step)\n",
        "        writer.add_scalar(\"data/num_missing\", num_missing, step)\n",
        "        writer.add_scalar(\"label/mean\", batch['label'].mean(), step)\n",
        "\n",
        "\n",
        "        # Increment step\n",
        "        step += 1\n",
        "        break\n",
        "\n",
        "    writer.flush()\n",
        "writer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!ls -lht ./runs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
