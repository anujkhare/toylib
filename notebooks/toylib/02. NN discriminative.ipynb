{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple Neural Network, using toylib!\n",
    "We repeat the same exercise that we did previously for learning jax, but building `toylib` along this time around!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "from toylib.nn import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up a simple regression problem. The problem is not very meaningful to solve by itself using such a model, but it allows us to get all the pieces in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# problem setup\n",
    "n = 120  # examlpes\n",
    "d = 10  # dimension\n",
    "\n",
    "# dummy data\n",
    "xs = np.random.normal(size=(n, d))\n",
    "weights_true = np.random.randint(0, 10, size=(d,))\n",
    "ys = np.dot(xs, weights_true) + np.random.normal(size=(n,))\n",
    "\n",
    "xs_train, xs_test = xs[:100], xs[100:]\n",
    "ys_train, ys_test = ys[:100], ys[100:]\n",
    "print(weights_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dummy `ys` are a linear transformation of the inputs. Let's try to fit a model on the train set to predict the values in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit\n",
    "def loss_function(model, xs, ys):\n",
    "    preds = jax.numpy.squeeze(jax.vmap(model)(xs))\n",
    "    return jax.numpy.mean((ys - preds) ** 2)  # L2 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = layers.Linear(d, 1, use_bias=False, key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encounter another jax specific error here. The jax.jit function works specficially on pytrees, but the class that we defined is not interpretable as one.\n",
    "\n",
    "We need to fix this.\n",
    "\n",
    "We follow the startegy defined in https://jax.readthedocs.io/en/latest/faq.html#strategy-3-making-customclass-a-pytree. There are other ways to achieve this and altogether different design options possible, we just choose this as a simple and flexible way to make progress.\n",
    "\n",
    "\n",
    "We rely on the abstract class `Module` and register it as a pytree node class.\n",
    "\n",
    "The class needs to distinguish between the `dynamic` elements (which need to interact with jax via `jit` & `grad`) vs the `static` elements:\n",
    "- Hyperparameters (like layer sizes) will be static\n",
    "- The actual weight arrays will be dynamic\n",
    "\n",
    "This is very doable for a single class by manually definiing each of these.\n",
    "\n",
    "For making this more generally useful, we define some helper functions to make a general `pytree` class that is understood by jax.\n",
    "\n",
    "We make some simplifying assumptions here:\n",
    "- All jax or numpy arrays in the modules will be parameters\n",
    "- Everything else is a hyperparameter to be treated as aux data.\n",
    "\n",
    "Some things are still unclear: what happens with nested modules?\n",
    "We shall deal with these at a later point.\n",
    "\n",
    "\n",
    "Now that we have a basic `Linear` module, we can define our first forward-backward pass using the `jax.value_and_grad` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, grads = jax.value_and_grad(loss_function)(model, xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grads` is now also an object of the same type `Linear`. This is because jax now treats `Linear` objects as\n",
    "pytree nodes. For each applicable child in the node, it will produce a grad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the grads, we still need to update the original model parameters.\n",
    "\n",
    "Here, we simply use `theta_new` = `theta` - `alpha * grads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_update(model, grads, learning_rate):\n",
    "    return jax.tree_map(lambda x, y: x - learning_rate * y, model, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tree_flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training loop\n",
    "# initial estimates\n",
    "\n",
    "# hyperparmeters\n",
    "max_steps = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "step = 0\n",
    "\n",
    "# until we reach max_steps or the loss doesn't change by <eps>\n",
    "while step < max_steps:\n",
    "    # compute model value and grad\n",
    "    loss, grads = jax.value_and_grad(loss_function)(model, xs, ys)\n",
    "    print(loss)\n",
    "    print(model.weights)\n",
    "    print(grads.weights)\n",
    "    model = apply_update(model, grads, learning_rate)\n",
    "    print(model.weights)\n",
    "\n",
    "    step += 1\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d268801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(weights_true, model.weights.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6b97af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
