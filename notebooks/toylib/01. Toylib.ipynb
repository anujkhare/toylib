{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### toylib: A simple Neural Network library in jax!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e50c0b",
   "metadata": {},
   "source": [
    "Let's build yet another jax neural networks library. Our primary aim is to keep things (relatively) simple and transparent and progressively building up complexity. Why build a library at all? Reuse - as we build more complex networks, it is important to have building blocks we can rely upon. The goal is not to compete with Flax or Haiku â€” but to rebuild their core ideas from scratch.\n",
    "\n",
    "I have been greatly inspired by the design of [equinox](https://github.com/patrick-kidger/equinox). Quoting from their Github page:\n",
    "- neural networks (or more generally any model), with easy-to-use PyTorch-like syntax;\n",
    "- filtered APIs for transformations;\n",
    "- useful PyTree manipulation routines;\n",
    "- advanced features like runtime errors;\n",
    "- and best of all, Equinox isn't a framework: everything you write in Equinox is compatible with anything else in JAX or the ecosystem.\n",
    "\n",
    "This is very close to my ideal library for sticking as close as possible to the core jax functionality. Having said that, equinox is still a feature complete library that holds its own against the likes of Flax and PyTorch.  We aim for a much simpler version inspired by Equinox with no bells and whistles.\n",
    "\n",
    "Jax transforms work very well with [Pytrees](https://docs.jax.dev/en/latest/pytrees.html). It is a natural choice then to represent all our model parameters as a Pytrees. An MLP with a couple of linear layers could be represented something like:\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    'layer1': {\n",
    "        'w': [0, 0, 1, 2],\n",
    "        'b': [1]\n",
    "    },\n",
    "    'layer2': {\n",
    "        'w': [-1, 2, 0, 3],\n",
    "        'b': [-1]\n",
    "    },\n",
    "}\n",
    "```\n",
    "\n",
    "This is simple and works natively with jax transforms, but this will quickly get out of hand as we develop more nested components.\n",
    "\n",
    "So, we try to come up with the `Module` abstraction. The Module is still a Pytree node, with:\n",
    "- trainable fields for parameters (weights, biases, etc.)\n",
    "- static fields for metadata (like hidden sizes, name scopes, etc.)\n",
    "- nested modules to enable composition\n",
    "- compatibility with jax.jit, jax.grad, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca06839",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jaxtyping\n",
    "import math\n",
    "import numpy as np\n",
    "import typing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up a simple linear regression in N variables. Analytic solutions exist for this problem, but we use this as the first toy example as we build up toylib!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8e5211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem setup\n",
    "n = 120  # examples\n",
    "d = 10  # dimensions\n",
    "\n",
    "# Generate some dummy data\n",
    "np.random.seed(31)\n",
    "xs = np.random.normal(size=(n, d))\n",
    "weights_true = np.random.randint(0, 10, size=(d,))\n",
    "ys = np.dot(xs, weights_true) + np.random.normal(size=(n,))\n",
    "\n",
    "xs_train, xs_test = xs[:100], xs[100:]\n",
    "ys_train, ys_test = ys[:100], ys[100:]\n",
    "print(weights_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bacfb4",
   "metadata": {},
   "source": [
    "Let's define a linear layer that does a single matrix multiplication and optionally adds a bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    pass\n",
    "\n",
    "class Linear(Module):\n",
    "    \"\"\"Defines a simple feedforward layer: which is a linear transformation. \"\"\"\n",
    "\n",
    "    # Trainable parameters\n",
    "    weights: jaxtyping.Array\n",
    "    bias: typing.Optional[jaxtyping.Array]\n",
    "\n",
    "    # Hyperparameters / metadata\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    use_bias: bool\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, use_bias: bool = True, *, key: jaxtyping.PRNGKeyArray) -> None:\n",
    "        # Split the random key for weights and bias\n",
    "        w_key, b_key = jax.random.split(key, 2)\n",
    "        \n",
    "        # We initialize the weights with a uniform distribution\n",
    "        lim = 1 / math.sqrt(in_features)\n",
    "        self.weights = jax.random.uniform(w_key, (in_features, out_features), minval=-lim, maxval=lim)\n",
    "        if use_bias:\n",
    "            self.bias = jax.random.uniform(b_key, (out_features,), minval=-lim, maxval=lim)\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = use_bias\n",
    "        self.key = key\n",
    "\n",
    "    def __call__(self, x: jaxtyping.Array) -> jaxtyping.Array:\n",
    "        x = jax.numpy.dot(x, self.weights)\n",
    "        if self.use_bias:\n",
    "            x = x + self.bias\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b418d89e",
   "metadata": {},
   "source": [
    "Let's initialize a linear layer to match our data and do a simple forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccdef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear(d, 1, use_bias=True, key=jax.random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c228adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(xs_train)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d041f5e",
   "metadata": {},
   "source": [
    "Looks good so far, let's define a loss function. We use the L2 loss (mean squared error) here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20da3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(model, xs, ys):\n",
    "    preds = jax.numpy.squeeze(model(xs))\n",
    "    return jax.numpy.mean((ys - preds) ** 2)  # L2 Loss\n",
    "\n",
    "print(loss_function(model, xs_train, ys_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed8d4a",
   "metadata": {},
   "source": [
    "For training the model, we would utilize jax's `value_and_grad` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7143d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.value_and_grad(loss_function)(model, xs_train, ys_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b327b",
   "metadata": {},
   "source": [
    "We encounter another jax specific error here. Jax transformations like jax.grad, jax.value_and_grad, jax.jit, etc., operate on JAX-compatible types: mostly jax.Arrays, Python containers like tuples/lists/dicts of JAX arrays, and custom types that are registered with JAX's PyTree machinery.\n",
    "\n",
    "We need to fix this.\n",
    "\n",
    "We follow the startegy defined in https://jax.readthedocs.io/en/latest/faq.html#strategy-3-making-customclass-a-pytree to define a custom Pytree node.\n",
    "\n",
    "Our `Module` class needs to distinguish between the `dynamic` elements (which need to interact with jax via `jit` & `grad`) vs the `static` elements:\n",
    "- Hyperparameters (like layer sizes) will be static\n",
    "- The actual weight arrays will be dynamic\n",
    "\n",
    "We make the following changes:\n",
    "- Use the `@register_pytree_node_class` decorator to tell Jax that Linear is also a Pytree\n",
    "- Add a `tree_flatten` method that separates the dynamic and static elements\n",
    "- Add a `tree_unflatten` method that constructs the object back given the dynamic and static elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26a5cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.tree_util import register_pytree_node_class\n",
    "\n",
    "@register_pytree_node_class\n",
    "class Linear(Module):\n",
    "    \"\"\"Defines a simple feedforward layer: which is a linear transformation. \"\"\"\n",
    "\n",
    "    # Trainable parameters\n",
    "    weights: jaxtyping.Array\n",
    "    bias: typing.Optional[jaxtyping.Array]\n",
    "\n",
    "    # Hyperparameters / metadata\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    use_bias: bool\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, use_bias: bool = True, *, key: jaxtyping.PRNGKeyArray) -> None:\n",
    "        # Split the random key for weights and bias\n",
    "        w_key, b_key = jax.random.split(key, 2)\n",
    "        \n",
    "        # We initialize the weights with a uniform distribution\n",
    "        lim = 1 / math.sqrt(in_features)\n",
    "        self.weights = jax.random.uniform(w_key, (in_features, out_features), minval=-lim, maxval=lim)\n",
    "        if use_bias:\n",
    "            self.bias = jax.random.uniform(b_key, (out_features,), minval=-lim, maxval=lim)\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = use_bias\n",
    "        self.key = key\n",
    "\n",
    "    def __call__(self, x: jaxtyping.Array) -> jaxtyping.Array:\n",
    "        x = jax.numpy.dot(x, self.weights)\n",
    "        if self.use_bias:\n",
    "            x = x + self.bias\n",
    "        return x\n",
    "\n",
    "    def tree_flatten(self) -> tuple:\n",
    "        params = [self.weights, self.bias]\n",
    "        static = {\n",
    "            'in_features': self.in_features,\n",
    "            'out_features': self.out_features,\n",
    "            'use_bias': self.use_bias,\n",
    "            'key': self.key,\n",
    "        }\n",
    "        return params, static\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, static, dynamic) -> 'Linear':\n",
    "        weights, bias = dynamic\n",
    "        in_features = static['in_features']\n",
    "        out_features = static['out_features']\n",
    "        use_bias = static['use_bias']\n",
    "        obj = cls(in_features, out_features, use_bias, key=static['key'])\n",
    "        obj.weights = weights\n",
    "        obj.bias = bias\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de117bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear(d, 1, use_bias=True, key=jax.random.PRNGKey(0))\n",
    "value, grad = jax.value_and_grad(loss_function)(model, xs_train, ys_train)\n",
    "print(value, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89becbf3",
   "metadata": {},
   "source": [
    "Great! We are able to get past the error and do a full forward and backward pass on the model.\n",
    "\n",
    "The `grads` is now also an object of the same type `Linear`. This is because jax now treats `Linear` objects as pytree nodes. For each applicable child in the node, it will produce a grad.\n",
    "\n",
    "How do we apply this gradient to get an update? Since `Linear` is now a PyTree, both model and grads share the same structure. We can use [jax.tree_utils.tree_map](https://docs.jax.dev/en/latest/_autosummary/jax.tree_util.tree_map.html) to walk both trees and apply the update element-wise.\n",
    "\n",
    "Here, we simply use `theta_new` = `theta` - `learning_rate * grads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a064d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(model, grad, learning_rate=0.01):\n",
    "    \"\"\"Update the model parameters using gradient descent.\"\"\"\n",
    "    return jax.tree_util.tree_map(lambda p, g: p - learning_rate * g, model, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500f28ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated = update(model, grad, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322686db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('model')\n",
    "jax.tree_util.tree_map(lambda x: print(x.shape), model)\n",
    "print('grad')\n",
    "jax.tree_util.tree_map(lambda x: print(x.shape), grad)\n",
    "print('updated')\n",
    "jax.tree_util.tree_map(lambda x: print(x.shape), updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f53431",
   "metadata": {},
   "source": [
    "This was relatively straightforward for the `Linear` class. However, it is not scalable to define these serialize/deserialize methods for every module we define.\n",
    "\n",
    "To make this more generally useful, we move these methods to the base `Module` class.\n",
    "\n",
    "We make some simplifying assumptions here:\n",
    "- All jax or numpy arrays in the modules will be parameters\n",
    "- Everything else is a hyperparameter to be treated as aux data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2504e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_array(x: typing.Any) -> bool:\n",
    "    return isinstance(\n",
    "        x, (jax.Array, np.ndarray, np.generic)\n",
    "    ) or hasattr(x, \"__jax_array__\")\n",
    "\n",
    "\n",
    "def _is_random_key(x: str) -> bool:\n",
    "    return x == 'key'\n",
    "\n",
    "@register_pytree_node_class\n",
    "class Module:\n",
    "    def tree_flatten(self) -> tuple:\n",
    "        params = []\n",
    "        param_keys = []\n",
    "        aux_data = dict()\n",
    "\n",
    "        # Look through each attribute in the object\n",
    "        for k, v in self.__dict__.items():\n",
    "            if _is_array(v) and not _is_random_key(k):\n",
    "                # trainable leaf param!\n",
    "                params.append(v)\n",
    "                param_keys.append(k)\n",
    "            else:\n",
    "                aux_data[k] = v\n",
    "\n",
    "        aux_data['param_keys'] = param_keys\n",
    "        return params, aux_data\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, static, dynamic) -> 'Linear':\n",
    "        # Create a new empty object\n",
    "        obj = object.__new__(cls)\n",
    "\n",
    "        # overwrite all of the children using the values in the given pytree\n",
    "        for k, v in zip(static['param_keys'], dynamic):\n",
    "            obj.__setattr__(k, v)\n",
    "\n",
    "        for k, v in static.items():\n",
    "            obj.__setattr__(k, v)\n",
    "\n",
    "        return obj\n",
    "\n",
    "@register_pytree_node_class\n",
    "class Linear(Module):\n",
    "    \"\"\"Defines a simple feedforward layer: which is a linear transformation. \"\"\"\n",
    "\n",
    "    # Trainable parameters\n",
    "    weights: jaxtyping.Array\n",
    "    bias: typing.Optional[jaxtyping.Array]\n",
    "\n",
    "    # Hyperparameters / metadata\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    use_bias: bool\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, use_bias: bool = True, *, key: jaxtyping.PRNGKeyArray) -> None:\n",
    "        # Split the random key for weights and bias\n",
    "        w_key, b_key = jax.random.split(key, 2)\n",
    "        \n",
    "        # We initialize the weights with a uniform distribution\n",
    "        lim = 1 / math.sqrt(in_features)\n",
    "        self.weights = jax.random.uniform(w_key, (in_features, out_features), minval=-lim, maxval=lim)\n",
    "        if use_bias:\n",
    "            self.bias = jax.random.uniform(b_key, (out_features,), minval=-lim, maxval=lim)\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = use_bias\n",
    "        self.key = key\n",
    "\n",
    "    def __call__(self, x: jaxtyping.Array) -> jaxtyping.Array:\n",
    "        x = jax.numpy.dot(x, self.weights)\n",
    "        if self.use_bias:\n",
    "            x = x + self.bias\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65951003",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear(d, 1, use_bias=True, key=jax.random.PRNGKey(0))\n",
    "value, grad = jax.value_and_grad(loss_function)(model, xs_train, ys_train)\n",
    "print(value, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a161aa8c",
   "metadata": {},
   "source": [
    "That works!\n",
    "\n",
    "The serializing function `tree_flatten` identifies all trainable parameters - which are assumed to be jax arrays, except for the random keys - and sets them as the \"dynamic\" elements in the pytree. Everything else, including the names of these parameters are added to the `aux_data` dict.\n",
    "All jax transforms will serialize our class using this function, operate on the dynamic elements, and then reconstruct the class using the `tree_unflatten` method.\n",
    "\n",
    "There is still one major feature that we haven't addressed yet: nested modules. \n",
    "\n",
    "Let's look at a simple MLP that uses two linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e09055",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class MLP(Module):\n",
    "    output_layer: Linear\n",
    "    layers: typing.List[Module]\n",
    "\n",
    "    in_features: int\n",
    "    hidden_dims: list[int]\n",
    "    out_features: int\n",
    "\n",
    "    def __init__(self, in_features: int, hidden_dims: list[int], out_features: int, *, key: jaxtyping.PRNGKeyArray) -> None:\n",
    "        # Split the random key for weights and bias\n",
    "        keys = jax.random.split(key, len(hidden_dims) + 1)\n",
    "\n",
    "        # Create the layers\n",
    "        layers = []\n",
    "        input_dim = in_features\n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            layer = Linear(input_dim, hidden_dim, key=keys[i])\n",
    "            layers.append(layer)\n",
    "            input_dim = hidden_dim\n",
    "\n",
    "        # Create the output layer\n",
    "        output_layer = Linear(input_dim, out_features, key=keys[-1])\n",
    "\n",
    "        self.layers = layers\n",
    "        self.output_layer = output_layer\n",
    "\n",
    "    def __call__(self, x: jaxtyping.Array) -> jaxtyping.Array:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            x = jax.nn.relu(x)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(d, [32, 32], 1, key=jax.random.PRNGKey(0))\n",
    "print(jax.value_and_grad(loss_function)(model, xs_train, ys_train))\n",
    "print(jax.tree_util.tree_flatten(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f71181a",
   "metadata": {},
   "source": [
    "At first glance, this looks okay since the forward pass works fine. However, a closer look at the output of `tree_flatten` reveals that the nested modules are not included in the dynamic elements at all and hence would be ignored by the JAX transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e36f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4c75ecd",
   "metadata": {},
   "source": [
    "This fails presently because we never serialize any nested modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b84c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_supported_container(x: typing.Any) -> bool:\n",
    "    return isinstance(x, (list, tuple))\n",
    "\n",
    "@register_pytree_node_class\n",
    "class Module:\n",
    "    def tree_flatten(self) -> tuple:\n",
    "        params = []\n",
    "        param_keys = []\n",
    "        aux_data = dict()\n",
    "\n",
    "        # Look through each attribute in the object\n",
    "        for k, v in self.__dict__.items():\n",
    "            if (\n",
    "                (_is_array(v) and not _is_random_key(k))\n",
    "                or isinstance(v, Module)\n",
    "                or (_is_supported_container(v) and all(isinstance(elem, Module) for elem in v))\n",
    "            ):\n",
    "                # trainable leaf param!\n",
    "                params.append(v)\n",
    "                param_keys.append(k)\n",
    "            else:\n",
    "                aux_data[k] = v\n",
    "\n",
    "        aux_data['param_keys'] = param_keys\n",
    "        return params, aux_data\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, static, dynamic) -> 'Linear':\n",
    "        # Create a new empty object\n",
    "        obj = object.__new__(cls)\n",
    "\n",
    "        # overwrite all of the children using the values in the given pytree\n",
    "        for k, v in zip(static['param_keys'], dynamic):\n",
    "            obj.__setattr__(k, v)\n",
    "\n",
    "        for k, v in static.items():\n",
    "            obj.__setattr__(k, v)\n",
    "\n",
    "        return obj\n",
    "\n",
    "@register_pytree_node_class\n",
    "class Linear(Module):\n",
    "    \"\"\"Defines a simple feedforward layer: which is a linear transformation. \"\"\"\n",
    "\n",
    "    # Trainable parameters\n",
    "    weights: jaxtyping.Array\n",
    "    bias: typing.Optional[jaxtyping.Array]\n",
    "\n",
    "    # Hyperparameters / metadata\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    use_bias: bool\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, use_bias: bool = True, *, key: jaxtyping.PRNGKeyArray) -> None:\n",
    "        # Split the random key for weights and bias\n",
    "        w_key, b_key = jax.random.split(key, 2)\n",
    "        \n",
    "        # We initialize the weights with a uniform distribution\n",
    "        lim = 1 / math.sqrt(in_features)\n",
    "        self.weights = jax.random.uniform(w_key, (in_features, out_features), minval=-lim, maxval=lim)\n",
    "        if use_bias:\n",
    "            self.bias = jax.random.uniform(b_key, (out_features,), minval=-lim, maxval=lim)\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = use_bias\n",
    "        self.key = key\n",
    "\n",
    "    def __call__(self, x: jaxtyping.Array) -> jaxtyping.Array:\n",
    "        x = jax.numpy.dot(x, self.weights)\n",
    "        if self.use_bias:\n",
    "            x = x + self.bias\n",
    "        return x\n",
    "\n",
    "@register_pytree_node_class\n",
    "class MLP(Module):\n",
    "    output_layer: Linear\n",
    "    layers: typing.List[Module]\n",
    "\n",
    "    in_features: int\n",
    "    hidden_dims: list[int]\n",
    "    out_features: int\n",
    "\n",
    "    def __init__(self, in_features: int, hidden_dims: list[int], out_features: int, *, key: jaxtyping.PRNGKeyArray) -> None:\n",
    "        # Split the random key for weights and bias\n",
    "        keys = jax.random.split(key, len(hidden_dims) + 1)\n",
    "\n",
    "        # Create the layers\n",
    "        layers = []\n",
    "        input_dim = in_features\n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            layer = Linear(input_dim, hidden_dim, key=keys[i])\n",
    "            layers.append(layer)\n",
    "            input_dim = hidden_dim\n",
    "\n",
    "        # Create the output layer\n",
    "        output_layer = Linear(input_dim, out_features, key=keys[-1])\n",
    "\n",
    "        self.layers = layers\n",
    "        self.output_layer = output_layer\n",
    "\n",
    "    def __call__(self, x: jaxtyping.Array) -> jaxtyping.Array:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            x = jax.nn.relu(x)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e592185",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(d, [32, 64], 1, key=jax.random.PRNGKey(0))\n",
    "print(jax.value_and_grad(loss_function)(model, xs_train, ys_train))\n",
    "jax.tree_util.tree_map(lambda x: print(x.shape), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb39a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jax.tree_util.tree_flatten(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd04a9",
   "metadata": {},
   "source": [
    "## Train a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training loop\n",
    "# initial estimates\n",
    "\n",
    "# hyperparmeters\n",
    "max_steps = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "step = 0\n",
    "\n",
    "# until we reach max_steps or the loss doesn't change by <eps>\n",
    "while step < max_steps:\n",
    "    # compute model value and grad\n",
    "    loss, grads = jax.value_and_grad(loss_function)(model, xs, ys)\n",
    "    print(loss)\n",
    "    print(model.weights)\n",
    "    print(grads.weights)\n",
    "    model = apply_update(model, grads, learning_rate)\n",
    "    print(model.weights)\n",
    "\n",
    "    step += 1\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4187c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
