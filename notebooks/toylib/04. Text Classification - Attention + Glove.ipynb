{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1S9viVuVaQi"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "from tensorboardX import SummaryWriter\n",
        "import chex\n",
        "import copy\n",
        "import dataclasses\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "from toylib import nn\n",
        "from toylib.data import imdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4Vf1onlcX4J"
      },
      "outputs": [],
      "source": [
        "#@title General hyperparms\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class Config:\n",
        "    batch_size: int = 128\n",
        "    embedding_dim: int = 100\n",
        "    max_tokens: int = 160\n",
        "\n",
        "    embeddings_path: str = 'glove.6B.100d.txt'\n",
        "\n",
        "    # training loop hyperparms\n",
        "    num_epochs: int = 5\n",
        "    learning_rate: float = 1e-2\n",
        "\n",
        "    # <CLS> token to be used for classification\n",
        "    # We use 'unk' because we don't have a <CLS> token in the vocab and we're not using the 'unk' token otherwise\n",
        "    cls_token: str = 'unk'\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "in_ = 10\n",
        "out_ = (8, 12)\n",
        "x = np.zeros((4, in_))\n",
        "\n",
        "nltk.download('punkt', download_dir='~/data/nltk_data')\n",
        "nltk.download('punkt_tab', download_dir='~/data/nltk_data')\n",
        "\n",
        "nltk.data.path.append('~/data/nltk_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load embeddings and dataset\n",
        "glove = imdb.load_glove_embeddings(config.embeddings_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxsA1NtfXxRe"
      },
      "outputs": [],
      "source": [
        "#@title Set up dataloaders: use IMDB movie sentiment reviews from HuggingFace datasets\n",
        "(\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    test_dataset,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    test_dataloader,\n",
        ") = imdb.load_dataset(\n",
        "    glove=glove,\n",
        "    batch_size=config.batch_size,\n",
        "    embedding_dim=config.embedding_dim,\n",
        "    max_tokens=config.max_tokens,\n",
        "    cls_token=config.cls_token,\n",
        ")\n",
        "print(f'Training batches: {len(train_dataset) / config.batch_size}')\n",
        "print(f'Val batches: {len(val_dataset) / config.batch_size}')\n",
        "print(f'Test batches: {len(test_dataset) / config.batch_size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "012zDTfjWRQy",
        "outputId": "76494fd1-3c93-4410-ef55-8657b5da9a62"
      },
      "outputs": [],
      "source": [
        "#@title Visualize a batch of data\n",
        "def visualize_samples(batch) -> pd.DataFrame:\n",
        "    missing = batch['embedding_missing'].sum(axis=1) - batch['num_pad']\n",
        "    return pd.DataFrame({\n",
        "        'labels': batch['label'],\n",
        "        'text': batch['raw_text'],\n",
        "        'missing': missing,\n",
        "        'num_tokens': batch['num_tokens'],\n",
        "    })\n",
        "\n",
        "batch = next(iter(train_dataloader))\n",
        "visualize_samples(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "For the exact formulation of attention, we  ()\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"../images/04.scaled-dot-product.png\" alt=\"Scaled Dot Product Attention\">\n",
        "</div>\n",
        "\n",
        "$$\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We implement a model inspired by the [BERT](https://arxiv.org/abs/1810.04805)-style self-attention based model architecture. Instead of using a learned tokenizer, as is common in all LLMs now, we continue to rely on the pre-trained and fixed Glove word embeddings. Further, we use a much smaller model and do not perform any self-supervised pre-training on a large text corpus.\n",
        "\n",
        "All these changes greatly reduce the scope of our initial approach, while still resulting in a relatively performant model.\n",
        "\n",
        "Similar to BERT, we allocate add a special token at the beginning of each example. Here, we use the `unk` token because it exists in the Glove vocabulary and we are not utilizing it in our present model. This differs from BERT, which uses the `<CLS>` token, because the lack of pre-training prevents the model from understanding the significance of this token.\n",
        "\n",
        "Our overall strategy is as follows:\n",
        "1. Add the `unk` token at the beginning of each example\n",
        "1. Use a few multi-head self attention layers to fuse/mix the input token embeddings\n",
        "1. Use the final layer embedding corresponding to the `unk` token as the final sentence representation\n",
        "1. Apply the output projection on the `unk` token embedding\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "For each self attention layer, we use the structure from the [Attention Is All You Need](https://arxiv.org/pdf/1706.03762) paper:\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"../images/04.mha.png\" alt=\"Multi Headed Attention Block\">\n",
        "</div>\n",
        "\n",
        "This paper used `h = 8` parallel heads in each attention block / layer. For each of these we use\n",
        "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
        "is similar to that of single-head attention with full dimensionality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45J_p5V0cM0Q"
      },
      "outputs": [],
      "source": [
        "#@title Define a model\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "@dataclasses.dataclass\n",
        "class SelfAttentionClassifier(nn.module.Module):\n",
        "    key: jax.random.PRNGKey\n",
        "\n",
        "    input_dim: int = 100\n",
        "    output_dim: int = 1\n",
        "\n",
        "    # Attention layers\n",
        "    num_layers: int = 2\n",
        "    qkv_dim: int = 256\n",
        "    num_heads: int = 4\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        # Generate keys\n",
        "        keys = jax.random.split(self.key, self.num_layers+2)\n",
        "\n",
        "        # Input projection\n",
        "        self.input_projection = nn.layers.Linear(in_features=self.input_dim, out_features=self.qkv_dim, key=keys[0])\n",
        "\n",
        "        # Self-attention layers\n",
        "        self.layers = []\n",
        "        for ix in range(self.num_layers):\n",
        "            self.layers.append(nn.attention.MultiHeadAttention(num_heads=self.num_heads, qkv_dim=self.qkv_dim, key=keys[ix+1]))\n",
        "\n",
        "        # Output projection\n",
        "        self.output_layer = nn.layers.Linear(in_features=self.qkv_dim, out_features=self.output_dim, key=keys[-1])\n",
        "\n",
        "    def __call__(self, x: jnp.ndarray, mask: Optional[jnp.ndarray] = None) -> ...:\n",
        "        chex.assert_rank(x, 2)  # [num_tokens, embed_dim] - batch should be vmapped\n",
        "        if mask is not None:\n",
        "            chex.assert_shape(mask, (x.shape[0],))\n",
        "\n",
        "        print('Input', x.shape)\n",
        "\n",
        "        # Input projection to project the embeddings to the model dimension\n",
        "        x = self.input_projection(x)\n",
        "        print('input projection', x.shape)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x, _ = layer(Q=x, K=x, V=x, mask=mask)\n",
        "        print('encoder output', x.shape)\n",
        "\n",
        "        # Apply the output projection on only the first token which corresponds to the <CLS> token\n",
        "        x = self.output_layer(x[0])\n",
        "        print('output projection', x.shape)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9Y8pzHbhXrg"
      },
      "outputs": [],
      "source": [
        "def loss_fn(model, batch):\n",
        "    x, y, mask = batch[\"embedding\"], batch[\"label\"], ~batch[\"embedding_missing\"]\n",
        "    logits = jax.vmap(model)(x, mask)\n",
        "\n",
        "    # Binary cross entropy from logits\n",
        "    log_probs = jax.nn.log_sigmoid(logits)\n",
        "    log_1minus_probs = jax.nn.log_sigmoid(-logits)  # log(1-sigmoid(x)) = log_sigmoid(-x)\n",
        "\n",
        "    loss = -(y * log_probs + (1 - y) * log_1minus_probs)\n",
        "\n",
        "    return jnp.mean(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir 4-attention/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model\n",
        "model = SelfAttentionClassifier(\n",
        "    input_dim=config.embedding_dim,\n",
        "    output_dim=1,\n",
        "    num_layers=2,\n",
        "    num_heads=4,\n",
        "    qkv_dim=256,\n",
        "    key=jax.random.PRNGKey(10)\n",
        ")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optax.adam(learning_rate=config.learning_rate)\n",
        "opt_state = optimizer.init(model)\n",
        "\n",
        "# Value and gradient\n",
        "loss_and_grad_fn = jax.value_and_grad(loss_fn)\n",
        "\n",
        "# TensorBoard writer\n",
        "writer = SummaryWriter(logdir=\"./4-attention/\" + time.strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "# Training loop\n",
        "step = 0\n",
        "# for epoch in range(config.num_epochs):\n",
        "orig_model = copy.deepcopy(model)\n",
        "for epoch in range(200):\n",
        "    for  batch in train_dataloader:\n",
        "        \n",
        "        loss_val, grads = loss_and_grad_fn(model, batch)\n",
        "        \n",
        "        # Apply gradients\n",
        "        updates, opt_state = optimizer.update(grads, opt_state)\n",
        "        leaves, _ = jax.tree_util.tree_flatten(updates)\n",
        "        model = optax.apply_updates(model, updates)\n",
        "\n",
        "        # Log to TensorBoard\n",
        "        writer.add_scalar(\"train/loss\", float(loss_val), step)\n",
        "        writer.add_scalar(\"train/learning_rate\", config.learning_rate, step)\n",
        "        writer.add_scalar(\"gradients/0/mean\", leaves[0].mean(), step)\n",
        "        writer.add_scalar(\"gradients/1/mean\", leaves[1].mean(), step)\n",
        "        writer.add_scalar(\"gradients/2/mean\", leaves[2].mean(), step)\n",
        "\n",
        "        num_missing = np.mean(batch['embedding_missing'].sum(axis=1) - batch['num_pad'])\n",
        "        writer.add_scalar(\"data/padding\", batch['num_pad'].mean(), step)\n",
        "        writer.add_scalar(\"data/num_missing\", num_missing, step)\n",
        "        writer.add_scalar(\"label/mean\", batch['label'].mean(), step)\n",
        "\n",
        "\n",
        "        # Increment step\n",
        "        step += 1\n",
        "        break\n",
        "\n",
        "    writer.flush()\n",
        "writer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!ls -lht ./runs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
